---
# =======================================================
# Stage A: Base OS setup
# =======================================================
- name: "SETUP: Base packages"
  hosts: all
  become: yes
  tags: [setup]
  roles:
    - k3s_setup

# =======================================================
# Stage B: K3s cluster
# =======================================================
- name: "SETUP: K3s Master"
  hosts: masters
  become: yes
  tags: [k3s]
  roles:
    - k3s_master

- name: "SETUP: K3s Workers"
  hosts: workers
  become: yes
  tags: [k3s]
  roles:
    - k3s_worker

# =======================================================
# Stage C: kubeconfig + kubectl/helm on DevOps node
# =======================================================
- name: "CONFIG: kubeconfig on master (for kubectl on master)"
  hosts: masters
  become: yes
  tags: [config]
  tasks:
    - name: Ensure .kube exists
      file:
        path: /root/.kube
        state: directory
        mode: "0700"

    - name: Copy kubeconfig
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /root/.kube/config
        remote_src: true
        mode: "0600"

    - name: Replace localhost with master public IP
      replace:
        path: /root/.kube/config
        regexp: '127\.0\.0\.1'
        replace: "{{ hostvars[inventory_hostname].ansible_host }}"

- name: "CONFIG: Install kubectl/helm on DevOps node + copy kubeconfig for Jenkins"
  hosts: devops
  become: yes
  tags: [config]
  tasks:
    - name: Install kubectl prerequisites
      apt:
        name:
          - curl
          - apt-transport-https
          - ca-certificates
        state: present
        update_cache: yes

    - name: Install kubectl (stable)
      shell: |
        curl -fsSL https://dl.k8s.io/release/stable.txt -o /tmp/k8s_version
        curl -fsSL "https://dl.k8s.io/release/$(cat /tmp/k8s_version)/bin/linux/amd64/kubectl" -o /usr/local/bin/kubectl
        chmod +x /usr/local/bin/kubectl
      args:
        creates: /usr/local/bin/kubectl

    - name: Install Helm
      shell: |
        curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      args:
        creates: /usr/local/bin/helm

    - name: Create Jenkins kubeconfig directory
      file:
        path: /var/lib/jenkins/.kube
        state: directory
        mode: "0755"

    - name: Fetch kubeconfig from master to controller
      fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /tmp/k3s_kubeconfig
        flat: yes
      delegate_to: "{{ groups['masters'][0] }}"

    - name: Write kubeconfig for Jenkins on DevOps node
      copy:
        src: /tmp/k3s_kubeconfig
        dest: /var/lib/jenkins/.kube/config
        mode: "0644"

    - name: Replace localhost with master public IP in Jenkins kubeconfig
      replace:
        path: /var/lib/jenkins/.kube/config
        regexp: '127\.0\.0\.1'
        replace: "{{ hostvars[groups['masters'][0]].ansible_host }}"
      ignore_errors: true

# =======================================================
# Stage D: Optional node labels (backend/frontend) for demo
# (robust: label by InternalIP -> nodeName mapping)
# =======================================================
- name: "CONFIG: Label worker nodes (optional)"
  hosts: masters
  become: yes
  tags: [label]
  tasks:
    - name: Gather facts from workers (to know their private IPs)
      setup:
      delegate_to: "{{ item }}"
      delegate_facts: true
      loop: "{{ groups['workers'] }}"

    - name: Build map InternalIP -> NodeName from cluster
      shell: |
        kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{range .status.addresses[?(@.type=="InternalIP")]}{.address}{end}{"\n"}{end}'
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      register: nodes_map
      changed_when: false

    - name: Set node_name_by_ip fact (ip -> nodeName)
      set_fact:
        node_name_by_ip: "{{ dict(nodes_map.stdout_lines | map('split', '\t') | map('reverse') | list) }}"

    - name: Apply labels role=<node_role> (match by worker private IP)
      vars:
        worker_ip: "{{ hostvars[item].ansible_default_ipv4.address | default(hostvars[item].ansible_host) }}"
        node_name: "{{ node_name_by_ip.get(worker_ip, '') }}"
        role_val: "{{ hostvars[item]['node_role'] | default('worker') }}"
      shell: |
        kubectl label node {{ node_name }} role={{ role_val }} --overwrite
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      loop: "{{ groups['workers'] }}"
      when: node_name != ''
      changed_when: false

    - name: Warn if worker could not be matched to a Kubernetes node
      vars:
        worker_ip: "{{ hostvars[item].ansible_default_ipv4.address | default(hostvars[item].ansible_host) }}"
      debug:
        msg: "Could not find Kubernetes node for worker={{ item }} internal_ip={{ worker_ip }}. Run: kubectl get nodes -o wide"
      loop: "{{ groups['workers'] }}"
      when: node_name_by_ip.get(worker_ip, '') == ''

# =======================================================
# Stage E: Deploy (infra + app)
# =======================================================
- name: "DEPLOY: Deploy infrastructure + application"
  hosts: masters
  become: yes
  tags: [deploy]
  tasks:
    - name: ">>> STEP 1: Deploy Infrastructure <<<"
      include_role:
        name: deploy_app
        tasks_from: infrastructure.yml
      tags: [infra]

    - name: ">>> STEP 2: Deploy Backend <<<"
      include_role:
        name: deploy_app
        tasks_from: deployment.yml
      vars:
        target_service: backend
        image_tag: "{{ backend_image_tag | default('latest') }}"
      tags: [backend]

    - name: ">>> STEP 3: Deploy Frontend <<<"
      include_role:
        name: deploy_app
        tasks_from: deployment.yml
      vars:
        target_service: frontend
        image_tag: "{{ frontend_image_tag | default('latest') }}"
      tags: [frontend]
